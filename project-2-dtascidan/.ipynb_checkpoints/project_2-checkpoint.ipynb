{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Project 2</center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Daniel lampert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linux Commands Used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Command to bring up cluster\n",
    "    - docker-compose up -d\n",
    "2. Command to create the kafka topic \n",
    "    - docker-compose exec mids bash -c \"kafkacat -C -b kafka:29092 -t assessments -o beginning -e\"\n",
    "3. Command to publish the assessments json data to the kafka topic using kafkacat\n",
    "    - docker-compose exec mids bash -c \"cat /w205/project-2-dtascidan/assessment-attempts-20180128-121051-nested.json | jq '.[]' -c | kafkacat -P -b kafka:29092 -t assessments\"\n",
    "4. Command to shutdown the cluster\n",
    "    - docker-compose down"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import needed libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create data frame by subscribing to Kafka Topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Read in data\n",
    "raw_assessments = spark.read.format(\"kafka\").option(\"kafka.bootstrap.servers\", \"kafka:29092\").option(\"subscribe\",\"assessments\").option(\"startingOffsets\", \"earliest\").option(\"endingOffsets\", \"latest\").load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cache data frame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[key: binary, value: binary, topic: string, partition: int, offset: bigint, timestamp: timestamp, timestampType: int]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_assessments.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert json data to string and store as a new data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assessments = raw_assessments.select(raw_assessments.value.cast('string'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract assessments from data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "extracted_assessments = assessments.rdd.map(lambda x: Row(**json.loads(x.value))).toDF()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write to HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "extracted_assessments.write.mode('overwrite').parquet(\"/tmp/extracted_assessments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save as temporary table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "extracted_assessments.registerTempTable('assessments')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|             keen_id|\n",
      "+--------------------+\n",
      "|5a6745820eb8ab000...|\n",
      "|5a674541ab6b0a000...|\n",
      "|5a67999d3ed3e3000...|\n",
      "|5a6799694fc7c7000...|\n",
      "|5a6791e824fccd000...|\n",
      "|5a67a0b6852c2a000...|\n",
      "|5a67b627cc80e6000...|\n",
      "|5a67ac8cb0a5f4000...|\n",
      "|5a67a9ba060087000...|\n",
      "|5a67ac54411aed000...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Select keen_id from temporary table assessments and display it with .show()\n",
    "spark.sql(\"select keen_id from assessments limit 10\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------------------------------------------------------+\n",
      "|    keen_timestamp|sequences[questions] AS `questions`[0][user_incomplete]|\n",
      "+------------------+-------------------------------------------------------+\n",
      "| 1516717442.735266|                                                   true|\n",
      "| 1516717377.639827|                                                  false|\n",
      "| 1516738973.653394|                                                  false|\n",
      "|1516738921.1137421|                                                  false|\n",
      "| 1516737000.212122|                                                  false|\n",
      "| 1516740790.309757|                                                  false|\n",
      "|1516746279.3801291|                                                  false|\n",
      "| 1516743820.305464|                                                  false|\n",
      "|  1516743098.56811|                                                  false|\n",
      "| 1516743764.813107|                                                  false|\n",
      "+------------------+-------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Display timestamp and completeness of questions\n",
    "spark.sql(\"select keen_timestamp, sequences.questions[0].user_incomplete from assessments limit 10\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unroll part of the json data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def my_lambda_sequences_id(x):\n",
    "    raw_dict = json.loads(x.value)\n",
    "    my_dict = {\"keen_id\" : raw_dict[\"keen_id\"], \"sequences_id\" : raw_dict[\"sequences\"][\"id\"]}\n",
    "    return Row(**my_dict)\n",
    "my_sequences = assessments.rdd.map(my_lambda_sequences_id).toDF()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write my_sequences to HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_sequences.write.mode('overwrite').parquet(\"/tmp/my_sequences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save as temporary table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_sequences.registerTempTable('sequences')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show some of the unrolled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|        sequences_id|\n",
      "+--------------------+\n",
      "|5b28a462-7a3b-42e...|\n",
      "|5b28a462-7a3b-42e...|\n",
      "|b370a3aa-bf9e-4c1...|\n",
      "|b370a3aa-bf9e-4c1...|\n",
      "|04a192c1-4f5c-4ac...|\n",
      "|e7110aed-0d08-4cb...|\n",
      "|5251db24-2a6e-424...|\n",
      "|066b5326-e547-4da...|\n",
      "|8ac691f8-8c1a-403...|\n",
      "|066b5326-e547-4da...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select sequences_id from sequences limit 10\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+--------------------+\n",
      "|             keen_id|    keen_timestamp|        sequences_id|\n",
      "+--------------------+------------------+--------------------+\n",
      "|5a17a67efa1257000...|1511499390.3836269|8ac691f8-8c1a-403...|\n",
      "|5a26ee9cbf5ce1000...|1512500892.4166169|9bd87823-4508-4e0...|\n",
      "|5a29dcac74b662000...|1512692908.8423469|e7110aed-0d08-4cb...|\n",
      "|5a2fdab0eabeda000...|1513085616.2275269|cd800e92-afc3-447...|\n",
      "|5a30105020e9d4000...|1513099344.8624721|8ac691f8-8c1a-403...|\n",
      "|5a3a6fc3f0a100000...| 1513779139.354213|e7110aed-0d08-4cb...|\n",
      "|5a4e17fe08a892000...|1515067390.1336551|9abd5b51-6bd8-11e...|\n",
      "|5a4f3c69cc6444000...| 1515142249.858722|083844c5-772f-48d...|\n",
      "|5a51b21bd0480b000...| 1515303451.773272|e7110aed-0d08-4cb...|\n",
      "|5a575a85329e1a000...| 1515674245.348099|25ca21fe-4dbb-446...|\n",
      "+--------------------+------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select a.keen_id, a.keen_timestamp, s.sequences_id from assessments a join sequences s on a.keen_id = s.keen_id limit 10\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unroll more json data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+\n",
      "|                  id|my_count|\n",
      "+--------------------+--------+\n",
      "|7a2ed6d3-f492-49b...|       1|\n",
      "|bbed4358-999d-446...|       2|\n",
      "|e6ad8644-96b1-461...|       3|\n",
      "|95194331-ac43-454...|       4|\n",
      "|95194331-ac43-454...|       1|\n",
      "|bbed4358-999d-446...|       2|\n",
      "|e6ad8644-96b1-461...|       3|\n",
      "|7a2ed6d3-f492-49b...|       4|\n",
      "|b9ff2e88-cf9d-4bd...|       1|\n",
      "|bec23e7b-4870-49f...|       2|\n",
      "+--------------------+--------+\n",
      "\n",
      "+--------------------+------------------+--------------------+\n",
      "|             keen_id|    keen_timestamp|                  id|\n",
      "+--------------------+------------------+--------------------+\n",
      "|5a17a67efa1257000...|1511499390.3836269|803fc93f-7eb2-412...|\n",
      "|5a17a67efa1257000...|1511499390.3836269|f3cb88cc-5b79-41b...|\n",
      "|5a17a67efa1257000...|1511499390.3836269|32fe7d8d-6d89-4db...|\n",
      "|5a17a67efa1257000...|1511499390.3836269|5c34cf19-8cfd-4f5...|\n",
      "|5a26ee9cbf5ce1000...|1512500892.4166169|0603e6f4-c3f9-4c2...|\n",
      "|5a26ee9cbf5ce1000...|1512500892.4166169|26a06b88-2758-45b...|\n",
      "|5a26ee9cbf5ce1000...|1512500892.4166169|25b6effe-79b0-4c4...|\n",
      "|5a26ee9cbf5ce1000...|1512500892.4166169|6de03a9b-2a78-46b...|\n",
      "|5a26ee9cbf5ce1000...|1512500892.4166169|aaf39991-fa83-470...|\n",
      "|5a26ee9cbf5ce1000...|1512500892.4166169|aab2e817-73dc-4ff...|\n",
      "+--------------------+------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def my_lambda_questions(x):\n",
    "    raw_dict = json.loads(x.value)\n",
    "    my_list = []\n",
    "    my_count = 0\n",
    "    for l in raw_dict[\"sequences\"][\"questions\"]:\n",
    "        my_count += 1\n",
    "        my_dict = {\"keen_id\" : raw_dict[\"keen_id\"], \"my_count\" : my_count, \"id\" : l[\"id\"]}\n",
    "        my_list.append(Row(**my_dict))\n",
    "    return my_list\n",
    "\n",
    "my_questions = assessments.rdd.flatMap(my_lambda_questions).toDF()\n",
    "\n",
    "my_questions.registerTempTable('questions')\n",
    "\n",
    "spark.sql(\"select id, my_count from questions limit 10\").show()\n",
    "\n",
    "spark.sql(\"select q.keen_id, a.keen_timestamp, q.id from assessments a join questions q on a.keen_id = q.keen_id limit 10\").show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write my_questions to HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_questions.write.mode('overwrite').parquet(\"/tmp/my_questions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unroll json data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|correct|total|\n",
      "+-------+-----+\n",
      "|      2|    4|\n",
      "|      1|    4|\n",
      "|      3|    4|\n",
      "|      2|    4|\n",
      "|      3|    4|\n",
      "|      5|    5|\n",
      "|      1|    1|\n",
      "|      5|    5|\n",
      "|      4|    4|\n",
      "|      0|    5|\n",
      "+-------+-----+\n",
      "\n",
      "+-----+\n",
      "|score|\n",
      "+-----+\n",
      "|  0.5|\n",
      "| 0.25|\n",
      "| 0.75|\n",
      "|  0.5|\n",
      "| 0.75|\n",
      "|  1.0|\n",
      "|  1.0|\n",
      "|  1.0|\n",
      "|  1.0|\n",
      "|  0.0|\n",
      "+-----+\n",
      "\n",
      "+-----------------+\n",
      "|        avg_score|\n",
      "+-----------------+\n",
      "|62.65699745547047|\n",
      "+-----------------+\n",
      "\n",
      "+-------------------+\n",
      "| standard_deviation|\n",
      "+-------------------+\n",
      "|0.31086692286170553|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def my_lambda_correct_total(x):\n",
    "    \n",
    "    raw_dict = json.loads(x.value)\n",
    "    my_list = []\n",
    "    \n",
    "    if \"sequences\" in raw_dict:\n",
    "        \n",
    "        if \"counts\" in raw_dict[\"sequences\"]:\n",
    "            \n",
    "            if \"correct\" in raw_dict[\"sequences\"][\"counts\"] and \"total\" in raw_dict[\"sequences\"][\"counts\"]:\n",
    "                    \n",
    "                my_dict = {\"correct\": raw_dict[\"sequences\"][\"counts\"][\"correct\"], \n",
    "                           \"total\": raw_dict[\"sequences\"][\"counts\"][\"total\"]}\n",
    "                my_list.append(Row(**my_dict))\n",
    "    \n",
    "    return my_list\n",
    "\n",
    "my_correct_total = assessments.rdd.flatMap(my_lambda_correct_total).toDF()\n",
    "\n",
    "my_correct_total.registerTempTable('ct')\n",
    "\n",
    "spark.sql(\"select * from ct limit 10\").show()\n",
    "\n",
    "spark.sql(\"select correct / total as score from ct limit 10\").show()\n",
    "\n",
    "spark.sql(\"select avg(correct / total)*100 as avg_score from ct limit 10\").show()\n",
    "\n",
    "spark.sql(\"select stddev(correct / total) as standard_deviation from ct limit 10\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write to HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_correct_total.write.mode('overwrite').parquet(\"/tmp/my_correct_total\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. What are the 3 most common exams?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|           exam_name|count_exam|\n",
      "+--------------------+----------+\n",
      "|        Learning Git|       394|\n",
      "|Introduction to P...|       162|\n",
      "|Introduction to J...|       158|\n",
      "+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "com_exams = spark.sql(\"select exam_name, COUNT(exam_name) as count_exam FROM assessments GROUP BY exam_name ORDER BY count_exam DESC limit 3\")\n",
    "com_exams.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 3 Most popular classes\n",
    "1. Learning Git\n",
    "2. Introduction to Python\n",
    "3. Introduction to Java"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. What are the 3 least popular exams?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|           exam_name|count_exam|\n",
      "+--------------------+----------+\n",
      "|Nulls, Three-valu...|         1|\n",
      "|Learning to Visua...|         1|\n",
      "|Native Web Apps f...|         1|\n",
      "+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rare_exams = spark.sql(\"select exam_name, COUNT(exam_name) as count_exam FROM assessments GROUP BY exam_name ORDER BY count_exam ASC limit 3\")\n",
    "rare_exams.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. What are the three most popular start hours for exams?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+\n",
      "|count_start|start_hour|\n",
      "+-----------+----------+\n",
      "|        224|        14|\n",
      "|        217|        13|\n",
      "|        211|        15|\n",
      "+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT HOUR(started_at) as start_hour FROM assessments\").registerTempTable('start_hour')\n",
    "spark.sql(\"SELECT COUNT(start_hour) as count_start, start_hour FROM start_hour GROUP BY start_hour ORDER BY count_start DESC limit 3\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The three most common start hours are \n",
    "1. 2:00pm\n",
    "2. 1:00pm\n",
    "3. 3:00pm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. What is the breakdown of passing to failing grades?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------+\n",
      "|(CAST(scalarsubquery() AS DOUBLE) / CAST(count(1) AS DOUBLE))|\n",
      "+-------------------------------------------------------------+\n",
      "|                                           0.4983206106870229|\n",
      "+-------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT correct/total as score FROM ct\").registerTempTable('sc')\n",
    "spark.sql(\"SELECT (SELECT COUNT(score) FROM sc WHERE score < 0.70)/COUNT(*) FROM sc\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almost 50% of students did not receive a passing grade!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
